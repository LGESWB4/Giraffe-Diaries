import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "CarrotAI/Llama-3.2-Rabbit-Ko-3B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# íŒ¨ë”© í† í° ì„¤ì • (ì—†ì„ ê²½ìš° ì˜¤ë¥˜ ë°©ì§€)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else tokenizer.unk_token

# ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
MAX_TOKENS = 2048

# ì´ˆê¸° ëŒ€í™” í”„ë¡¬í”„íŠ¸ (ì‹œìŠ¤í…œ ì„¤ì •)
system_prompt = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ê°ì •ì„ ê³µê°í•˜ë©° ë”°ëœ»í•œ ë§íˆ¬ë¡œ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ê°ì„± ì±—ë´‡ì…ë‹ˆë‹¤.
í•­ìƒ **ì¡´ëŒ“ë§**ì„ ì‚¬ìš©í•˜ë©°, ì‚¬ìš©ìì˜ ê°ì •ì„ ì¡´ì¤‘í•˜ëŠ” **ì¹œì ˆí•˜ê³  ë¶€ë“œëŸ¬ìš´ ëŒ€í™”**ë¥¼ ë‚˜ëˆ„ì„¸ìš”.
ë„ˆë¬´ ê¸¸ì§€ ì•Šì§€ë§Œ, **ì •ì„œì ì¸ êµê°ì„ ìœ ì§€í•˜ë©° ìì—°ìŠ¤ëŸ½ê²Œ ì´ì•¼ê¸°í•˜ì„¸ìš”**.

1ï¸âƒ£ **ê³µê° ìš°ì„ **: ì‚¬ìš©ìê°€ í˜ë“¤ë©´ "ì •ë§ í˜ë“¤ì—ˆê² ì–´ìš”.", ê¸°ì˜ë©´ "ì •ë§ ì¢‹ì€ í•˜ë£¨ì˜€ë„¤ìš”!" ë“± **ê°ì •ì— ë§ëŠ” ë°˜ì‘**ì„ í•˜ì„¸ìš”.
2ï¸âƒ£ **ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸**: ì‚¬ìš©ìê°€ ë” ì´ì•¼ê¸°í•˜ê³  ì‹¶ê²Œ **ë¶€ë“œëŸ¬ìš´ ì§ˆë¬¸**ì„ ë˜ì§€ì„¸ìš”.
3ï¸âƒ£ **ë¶€ë‹´ ì—†ëŠ” ëŒ€í™”**: ë¶„ì„ì ì´ê±°ë‚˜ ë”±ë”±í•œ ë§íˆ¬ ëŒ€ì‹ , **ì¹œêµ¬ì²˜ëŸ¼ ë‹¤ì •í•œ ë§íˆ¬**ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

<|eot_id|>
"""

# ì´ˆê¸° ëŒ€í™” ê¸°ë¡ (ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬)
conversation_history = [system_prompt]

def trim_conversation():
    """ìµœëŒ€ í† í° ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì•ë¶€ë¶„ì„ ì‚­ì œí•˜ì—¬ ë§¥ë½ ìœ ì§€"""
    while True:
        encoded = tokenizer("".join(conversation_history), return_tensors="pt")
        if encoded.input_ids.shape[1] <= MAX_TOKENS:
            break  # í† í° ê¸¸ì´ê°€ ì´ˆê³¼í•˜ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
        conversation_history.pop(1)  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ìœ ì§€í•œ ì±„ ê°€ì¥ ì˜¤ë˜ëœ ë©”ì‹œì§€ ì‚­ì œ

# ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í™” ì§„í–‰
while True:
    try:
        user_input = input("ğŸ§‘â€ğŸ’» ì‚¬ìš©ì: ").strip()

        # ì¢…ë£Œ ì¡°ê±´
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ›‘ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        conversation_history.append(f"<|start_header_id|>user<|end_header_id|>\n\n{user_input}<|eot_id|>\n")
        conversation_history.append("<|start_header_id|>assistant<|end_header_id|>\n")  # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì‹œì‘

        # ëŒ€í™” ê¸°ë¡ ê¸¸ì´ ì¡°ì ˆ
        trim_conversation()

        # ì…ë ¥ ë³€í™˜ (í† í¬ë‚˜ì´ì € ì ìš©)
        inputs = tokenizer("".join(conversation_history), return_tensors="pt", padding=True, truncation=True)
        inputs = {key: value.to(model.device) for key, value in inputs.items()}  # GPU ì´ë™

        # ëª¨ë¸ ì˜ˆì¸¡
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_length=inputs["input_ids"].shape[1] + 256,  # í˜„ì¬ ì…ë ¥ ê¸¸ì´ + ì˜ˆìƒ ì‘ë‹µ ê¸¸ì´
                do_sample=True,  # í™•ë¥ ì  ìƒ˜í”Œë§ í™œì„±í™” (ë³´ë‹¤ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ)
                temperature=0.7,  # ì°½ì˜ì„± ì¡°ì ˆ
                top_p=0.9,  # ìƒìœ„ p% í™•ë¥ ì˜ í† í°ë§Œ ê³ ë ¤
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # ì‘ë‹µ ë””ì½”ë”©
        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ì¶œ
        assistant_response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        assistant_response = assistant_response.split("<|eot_id|>")[0].strip()  # ë íƒœê·¸ ì œê±°

        # ì‘ë‹µ ì¶œë ¥
        print(f"ğŸ¤– ì–´ì‹œìŠ¤í„´íŠ¸: {assistant_response}")

        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        conversation_history.append(f"{assistant_response}<|eot_id|>\n")

    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
