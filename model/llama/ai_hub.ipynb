{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qai-hub==0.23.0\n",
    "!pip install \"qai-hub-models[llama-v3-2-3b-chat-quantized]\"==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.4.1 torchvision torchaudio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!pip install transformers huggingface_hub -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/quic/ai-hub-apps/tree/main/tutorials/llm_on_genie\n",
    "# https://github.com/quic/ai-hub-apps/tree/main/apps/android/ChatApp\n",
    "\n",
    "\n",
    "'''\n",
    "// \"size\": 2048, // original: 4096 \n",
    "\n",
    "// Generation\t        soc_model\tdsp_arch\n",
    "// Snapdragon® Gen 3\t57\t        v75\n",
    "// Snapdragon® 8 Elite\t69\t        v79\n",
    "\n",
    "ssh -i C:\\Users\\munsi\\.ssh\\id_ed25519 -o StrictHostKeychecking=no -o UserKnownHostsFile=/dev/null -L 5037:ssh-196203-svc.ssh.svc.cluster.local:5037 -N sshtunnel@user.ssh.qdc.qualcomm.com\n",
    "\n",
    "cd /data/local/tmp/genie_bundle\n",
    "cp ./hexagon-v79/unsigned/* ./\n",
    "cp ./aarch64-android/* ./\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/moons98.in/miniconda3/envs/llama_new/lib/python3.10/site-packages/qai_hub/_cli.py:370: UserWarning: Overwriting configuration: /home/moons98.in/.qai_hub/client.ini (previous configuration saved to /home/moons98.in/.qai_hub/client.ini.bak)\n",
      "  warnings.warn(\n",
      "qai-hub configuration saved to /home/moons98.in/.qai_hub/client.ini\n",
      "==================== /home/moons98.in/.qai_hub/client.ini ====================\n",
      "[api]\n",
      "api_token = 5773c1176748d18ceca8586a2fa9adebf54765ca\n",
      "api_url = https://app.aihub.qualcomm.com\n",
      "web_url = https://app.aihub.qualcomm.com\n",
      "verbose = True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set tokens\n",
    "!qai-hub configure --api_token <your_token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download llama - write authority is needed to download model\n",
    "!rm -rf ~/.cache/huggingface\n",
    "# !huggingface-cli download unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-llama/Llama-3.2-3B-Instruct # llama3.2 ori\n",
    "# CarrotAI/Llama-3.2-Rabbit-Ko-3B-Instruct # llama3.2 ko\n",
    "# unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit # llama3.2 4bit quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qai_hub as hub\n",
    "hub.get_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m qai_hub_models.models.llama_v3_2_3b_chat_quantized.export --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/moons98.in/Giraffe-Diaries/model/llama/snapdragon-8-gen-3\n",
    "!mkdir \"/home/moons98.in/Giraffe-Diaries/model/llama/snapdragon-8-gen-3/ko-llama-re2\"\n",
    "!python -m qai_hub_models.models.llama_v3_2_3b_chat_quantized.export --huggingface-model-name \"CarrotAI/Llama-3.2-Rabbit-Ko-3B-Instruct\" --context-length 2048 --chipset \"qualcomm-snapdragon-8gen3\" --skip-inferencing --skip-profiling --output-dir ko-llama-re2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/moons98.in/Giraffe-Diaries/model/llama/snapdragon-8-elite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/moons98.in/Giraffe-Diaries/model/llama/snapdragon-8-elite/quant-re’: File exists\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/qai_hub_models/models/llama_v3_2_3b_chat_quantized/export.py\", line 60, in <module>\n",
      "    main()\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/qai_hub_models/models/llama_v3_2_3b_chat_quantized/export.py\", line 48, in main\n",
      "    export_model(\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/qai_hub_models/models/_shared/llama3/export.py\", line 165, in export_model\n",
      "    model = model_cls.from_pretrained(sequence_length=seq_len, **model_params)\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/qai_hub_models/models/llama_v3_2_3b_chat_quantized/model.py\", line 81, in from_pretrained\n",
      "    return cls(\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/qai_hub_models/models/llama_v3_2_3b_chat_quantized/model.py\", line 36, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/qai_hub_models/models/_shared/llama3/model.py\", line 388, in __init__\n",
      "    model = modeling_llama.LlamaForCausalLM.from_pretrained(\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3436, in from_pretrained\n",
      "    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/quantizers/auto.py\", line 169, in merge_quantization_configs\n",
      "    quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/quantizers/auto.py\", line 99, in from_dict\n",
      "    return target_cls.from_dict(quantization_config_dict)\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 99, in from_dict\n",
      "    config = cls(**config_dict)\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 402, in __init__\n",
      "    self.post_init()\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 460, in post_init\n",
      "    if self.load_in_4bit and not version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/importlib/metadata/__init__.py\", line 996, in version\n",
      "    return distribution(distribution_name).version\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/importlib/metadata/__init__.py\", line 969, in distribution\n",
      "    return Distribution.from_name(distribution_name)\n",
      "  File \"/home/moons98.in/miniconda3/envs/llama/lib/python3.10/importlib/metadata/__init__.py\", line 548, in from_name\n",
      "    raise PackageNotFoundError(name)\n",
      "importlib.metadata.PackageNotFoundError: No package metadata was found for bitsandbytes\n"
     ]
    }
   ],
   "source": [
    "%cd /home/moons98.in/Giraffe-Diaries/model/llama/snapdragon-8-elite\n",
    "!mkdir \"/home/moons98.in/Giraffe-Diaries/model/llama/snapdragon-8-elite/quant-re\"\n",
    "!python -m qai_hub_models.models.llama_v3_2_3b_chat_quantized.export --huggingface-model-name \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\" --context-length 2048 --device \"Snapdragon 8 Elite QRD\" --skip-inferencing --skip-profiling --output-dir quant-re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device : \n",
    "#       For Android - \"Samsung Galaxy S24 Ultra\" \"Snapdragon 8 Elite QRD\"\n",
    "#       For Windows - \"Samsung Galaxy S24 Ultra\" \"Snapdragon 8 Elite CRD\"\n",
    "\n",
    "# --context-length 2048 for 3B model\n",
    "!python -m qai_hub_models.models.llama_v3_2_3b_chat_quantized.export --context-length 2048 --device \"Snapdragon 8 Elite QRD\" --skip-inferencing --skip-profiling --output-dir Llama-3.2-Rabbit-Ko-3B-Instruct\n",
    "# !python -m qai_hub_models.models.llama_v3_2_3b_chat_quantized.export --context-length 2048 --chipset \"qualcomm-snapdragon-8gen3\" --skip-inferencing --skip-profiling --output-dir genie_8gen3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove intermediate assets\n",
    "!rm -rf Llama-3.2-3B-Instruct-unsloth-bnb-4bit/{prompt,token}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"/home/moons98.in/Giraffe-Diaries/model/llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "llama3 prompt :\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ai-hub-apps\n",
    "!cp ./ai-hub-apps/tutorials/llm_on_genie/configs/htp/htp_backend_ext_config.json.template genie_8gen3/htp_backend_ext_config.json # soc 선택 필요\n",
    "!cp ai-hub-apps/tutorials/llm_on_genie/configs/genie/llama_v3_2_3b_chat_quantized.json genie_8gen3/genie_config.json # build 시 context_size 옵션 사용했으면 -> size option 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qairt\n",
    "!cp $QNN_SDK_ROOT/lib/hexagon-v75/unsigned/* genie_8gen3 # For 8 Gen 3\n",
    "!cp $QNN_SDK_ROOT/lib/hexagon-v79/unsigned/* genie_8gen3 # For 8 Elite\n",
    "\n",
    "# common\n",
    "!cp $QNN_SDK_ROOT/lib/aarch64-android/* genie_8gen3\n",
    "!cp $QNN_SDK_ROOT/bin/aarch64-android/genie-t2t-run genie_8gen3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 moons98.in moons98.in 752M Mar 13 01:13 ./llama3.2_bundle/llama_v3_2_3b_chat_quantized_part_1_of_3.bin\n",
      "-rw-r--r-- 1 moons98.in moons98.in 687M Mar 13 01:13 ./llama3.2_bundle/llama_v3_2_3b_chat_quantized_part_2_of_3.bin\n",
      "-rw-r--r-- 1 moons98.in moons98.in 1.1G Mar 13 01:38 ./llama3.2_bundle/llama_v3_2_3b_chat_quantized_part_3_of_3.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./genie_bundle_rebuild/llama_v3_2_3b_chat_quantized_part_1_of_3.bin\n",
    "!ls -lh ./genie_bundle_rebuild/llama_v3_2_3b_chat_quantized_part_2_of_3.bin\n",
    "!ls -lh ./genie_bundle_rebuild/llama_v3_2_3b_chat_quantized_part_3_of_3.bin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
