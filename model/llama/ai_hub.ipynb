{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"qai_hub_models[llama-v3_2-3b-chat-quantized]\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers huggingface_hub -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.4.1 torchvision torchaudio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"huggingface_hub[cli]\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/quic/ai-hub-apps/tree/main/tutorials/llm_on_genie\n",
    "# https://github.com/quic/ai-hub-apps/tree/main/apps/android/ChatApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokens\n",
    "!qai-hub configure --api_token <your_token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download llama - write authority is needed to download model;;\n",
    "!huggingface-cli download meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device : \n",
    "#       For Android - \"Snapdragon 8 Gen 3 QRD\" \"Snapdragon 8 Elite QRD\"\n",
    "#       For Windows - \"Snapdragon 8 Gen 3 CRD\" \"Snapdragon 8 Elite CRD\"\n",
    "\n",
    "# --context-length 2048 for 3B model\n",
    "!python -m qai_hub_models.models.llama_v3_2_3b_chat_quantized.export --device \"Snapdragon 8 Elite QRD\" --skip-inferencing --skip-profiling --output-dir llama3.2_bundle --context-length 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove intermediate assets\n",
    "!rm -rf genie_bundle/{prompt,token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qai_hub as hub\n",
    "\n",
    "name = 'custom4_dw_unet'\n",
    "torch_model = DWCustomUNet2(in_channels=1, out_channels=1)\n",
    "torch_model.eval()\n",
    "\n",
    "# Step 1: Trace model\n",
    "input_shape = (1, 1, 512, 512)\n",
    "example_input = torch.rand(input_shape)\n",
    "traced_torch_model = torch.jit.trace(torch_model, example_input)\n",
    "\n",
    "# Step 2: Compile model\n",
    "compile_job = hub.submit_compile_job( # AP에 최적화된 형태로 compile\n",
    "    model=traced_torch_model,\n",
    "    device=hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    input_specs=dict(image=input_shape),\n",
    ")\n",
    "\n",
    "# Step 3: Profile on cloud-hosted device\n",
    "target_model = compile_job.get_target_model()\n",
    "profile_job = hub.submit_profile_job( # compile된 모델을 가져와서 실제 HW에 올려서 prifiling\n",
    "    model=target_model,\n",
    "    device=hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    ")\n",
    "\n",
    "random_input = np.random.rand(1, 1, 512, 512).astype(np.float32)\n",
    "\n",
    "# Run inference using the on-device model on the input image\n",
    "inference_job = hub.submit_inference_job(\n",
    "    model=target_model,\n",
    "    device=hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    inputs=dict(image=[random_input]),\n",
    ")\n",
    "\n",
    "# # Step 6: Download model\n",
    "# target_model = compile_job.get_target_model()\n",
    "# target_model.download(f\"{name}.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
