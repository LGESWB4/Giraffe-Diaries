import 'dart:convert';
import 'dart:io';
import 'package:llama_library/io/models/model_params.dart';
import 'package:llama_library/llama_library.dart';
import 'package:llama_library/scheme/scheme/api/api.dart';
import 'package:llama_library/scheme/scheme/respond/update_llama_library_message.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';
import 'package:path_provider/path_provider.dart';
import 'modelload.dart';
import 'package:llama_library/io/models/context_params.dart';
import 'package:llama_library/io/models/sampler_params.dart';
import 'package:llama_library/io/models/model_params.dart';
import 'package:llama_library/llama_library.dart';

final systemPrompt_emotion = jsonEncode({
    "role": "system",
    "content": "assistantëŠ” ê°ì • ë¶„ì„ ì „ë¬¸ê°€ **ê¸°ë¦°AI**ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ê°€ì¥ ì ì ˆí•œ ê°ì • 1ê°œì™€ í•µì‹¬ ë‹¨ì–´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
    "ğŸ”¹ **ê°ì • ì„ íƒ:**\n"
    "ë‹¤ìŒ ëª©ë¡ì—ì„œ **ê°€ì¥ ì ì ˆí•œ ê°ì • 1ê°œ**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n"
    "['ê¸°ì¨', 'í–‰ë³µ', 'ë§Œì¡±', 'ì• ì • í‘œí˜„', 'ì¦ê±°ì›€', 'ìì‹ ê°', 'ë†€ëŒ', 'ìŠ¬í””', 'ìš°ìš¸', 'ê±±ì •', 'ë¶ˆë§Œ', 'í™”ë‚¨', 'ë‘ë ¤ì›€', 'ê¸´ì¥', 'ì‹¤ë§', 'í˜¼ë€', 'ì•„í””']\n\n"
    "ğŸ”¹ **í•µì‹¬ ë‹¨ì–´:**\n"
    "ì¼ê¸°ì˜ ì£¼ì œë¥¼ ë°˜ì˜í•˜ëŠ” **ìµœëŒ€ 4ê°œì˜ í‚¤ì›Œë“œ**ë§Œì„ ì„ ì •í•˜ì„¸ìš”.\n\n"
    "ğŸ“Œ **ì´ ì‘ë‹µ ê¸¸ì´ëŠ” 50ë‹¨ì–´ ì´í•˜ë¡œ ì œí•œí•˜ì„¸ìš”.**\n"
    "ğŸ“Œ **ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.**\n"
    "```json\n"
    "{\n"
    "    \"Emotion\": \"ê°ì •\",\n"
    "    \"Keywords\": [\"ë‹¨ì–´1\", \"ë‹¨ì–´2\", \"ë‹¨ì–´3\", \"ë‹¨ì–´4\"]\n"
    "}\n"
    "```\n"
});

final systemPrompt_chat = jsonEncode({
    "role": "system",
    "content": "assistantëŠ” ê°ì •ì„ ê³µê°í•˜ë©° ëŒ€í™”í•˜ëŠ” ê°ì„± ì±—ë´‡ **ê¸°ë¦°AI**ì…ë‹ˆë‹¤.\n"
    "í•­ìƒ **ì¡´ëŒ“ë§**ì„ ì‚¬ìš©í•˜ê³ , ë”°ëœ»í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ëŒ€í™”í•˜ì„¸ìš”.\n\n"
    "ğŸ”¹ **ëŒ€í™” ì›ì¹™:**\n"
    "1ï¸âƒ£ **ê³µê° í‘œí˜„:** ì‚¬ìš©ìì˜ ê°ì •ì— ë§ê²Œ 'ì •ë§ í˜ë“œì…¨ê² ì–´ìš”.', 'ì¢‹ì€ í•˜ë£¨ì˜€ë„¤ìš”!'ì²˜ëŸ¼ ì ì ˆíˆ ë°˜ì‘í•˜ì„¸ìš”.\n"
    "2ï¸âƒ£ **ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸:** ì‚¬ìš©ìê°€ ë” ì´ì•¼ê¸°í•˜ê³  ì‹¶ê²Œ ë¶€ë“œëŸ¬ìš´ ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”.\n"
    "3ï¸âƒ£ **ë¶€ë‹´ ì—†ëŠ” ëŒ€í™”:** ì¹œêµ¬ì²˜ëŸ¼ ë‹¤ì •í•œ ë§íˆ¬ë¥¼ ìœ ì§€í•˜ì„¸ìš”.\n"
    "4ï¸âƒ£ **ê°œì¸ì •ë³´ ë³´í˜¸:** ì´ë¦„, ìœ„ì¹˜ ë“± ê°œì¸ ì •ë³´ë¥¼ ìš”ì²­í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
    "ğŸ“Œ **ì‘ë‹µì€ 100ë‹¨ì–´ ì´í•˜ë¡œ ì‘ì„±í•˜ì„¸ìš”.**\n\n"
    "ğŸ”¹ **ì˜ˆì‹œ:**\n"
    "ì‚¬ìš©ì: ì˜¤ëŠ˜ ë„ˆë¬´ í”¼ê³¤í•´ìš”...\n"
    "â†’ **ê¸°ë¦°AI**: ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”. í•˜ë£¨ ì¢…ì¼ ë°”ì˜ì…¨ë‚˜ìš”? ğŸ˜¢\n\n"
    "ì‚¬ìš©ì: ì¹œêµ¬ë‘ ë¹µì§‘ ê°”ì–´ìš”!\n"
    "â†’ **ê¸°ë¦°AI**: ìš°ì™€! ì–´ë–¤ ë¹µì´ ê°€ì¥ ë§›ìˆì—ˆë‚˜ìš”? ğŸ¥\n\n"
    "ì‚¬ìš©ì: ê¸°ë¶„ì´ ì‹±ìˆ­ìƒìˆ­í•´ìš”.\n"
    "â†’ **ê¸°ë¦°AI**: ê·¸ëŸ´ ë•Œ ìˆì£ . ğŸ˜Œ í˜¹ì‹œ ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆë‚˜ìš”?"
});


String model_name = "Llama-3.2-Rabbit-Ko-3B-Instruct.i1-Q4_K_S.gguf";
//String model_name = "ktdsbaseLM-v0.13-onbased-llama3.1-Q4_K_M.gguf";

class LlamaChatService {
  final LlamaLibrary llamaLibrary;
  String currentResponse = '';
  final Function(String) onResponseUpdate;

  LlamaChatService({required this.onResponseUpdate})
      : llamaLibrary = LlamaLibrary(
          sharedLibraryPath: "libllama.so",
          invokeParametersLlamaLibraryDataOptions:
              InvokeParametersLlamaLibraryDataOptions(
            invokeTimeOut: Duration(minutes: 10),
            isThrowOnError: false,
          ),
        ) {
    _initialize();
  }

  Future<void> _initialize() async {
    await llamaLibrary.ensureInitialized();
    await llamaLibrary.initialized();
    await _loadModel();

  }

  Future<void> _loadModel() async {
    final Directory appDir = await getApplicationDocumentsDirectory();
    debugPrint('ì•± ë¡œì»¬ ì €ì¥ì†Œ ê²½ë¡œ: ${appDir.path}');
    String modelPath = "${appDir.path}/$model_name";

    if (await File(modelPath).exists()) {
      debugPrint("âœ… ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤: $modelPath");
    } else {
      await model_move();
      //debugPrint("âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.");
      debugPrint("âœ… ëª¨ë¸ íŒŒì¼ì´ ë³µì‚¬ì¤‘: $modelPath");
    }

    final res = await llamaLibrary.invoke(
      invokeParametersLlamaLibraryData: InvokeParametersLlamaLibraryData(
        parameters: LoadModelFromFileLlamaLibrary.create(
          model_file_path: modelPath,
        ),
        isVoid: false,
        extra: null,
        invokeParametersLlamaLibraryDataOptions: null,
      ),
    );

    final res = await llamaLibrary.invoke(
      invokeParametersLlamaLibraryData: InvokeParametersLlamaLibraryData(
        parameters: .create(
          model_file_path: modelPath,
        ),
        isVoid: false,
        extra: null,
        invokeParametersLlamaLibraryDataOptions: null,
      ),
    );

    print("res: $res");
    if (res["@type"] == "ok") {
      print("Success load Model");
    } else {
      print("Failed load Model");
      throw Exception("Failed to load model");
    }
  }

  Future<void> sendMessage(String text, final chatHistory) async {
    final userPrompt = text;

    chatHistory.addMessage(
      role: LlamaLibraryRole.user,
      content: userPrompt
    );

    String chatHistory_text = systemPrompt_emotion;

    currentResponse = '';
    String utf8response = '';
    String jsonresponse = '';

    llamaLibrary.on(
      eventType: llamaLibrary.eventUpdate,
      onUpdate: (data) {
        final update = data.update;
        if (update is UpdateLlamaLibraryMessage) {
          if (update.is_done == false) {
            if (update.text != null) {
              try {
                // ì›ë³¸ í…ìŠ¤íŠ¸ ì¶œë ¥
                //debugPrint('ì›ë³¸ í…ìŠ¤íŠ¸: ${update.text}');
                
                // UTF-8 ë””ì½”ë”© ì‹œë„
                try {
                  final String utf8Decoded = utf8.decode(update.text!.codeUnits);
                  debugPrint('UTF-8 ë””ì½”ë”© ê²°ê³¼: $utf8Decoded');
                  utf8response = utf8Decoded;
                } catch (e) {
                  debugPrint('UTF-8 ë””ì½”ë”© ì‹¤íŒ¨: $e');
                }

                // JSON ë””ì½”ë”© ì‹œë„
                try {
                  final dynamic jsonDecoded = jsonDecode(update.text!);
                  debugPrint('JSON ë””ì½”ë”© ê²°ê³¼: $jsonDecoded');
                  jsonresponse = jsonDecoded;
                } catch (e) {
                  debugPrint('JSON ë””ì½”ë”© ì‹¤íŒ¨: $e');
                }

                // í˜„ì¬ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                currentResponse += utf8response;
                onResponseUpdate(currentResponse);
              } catch (e) {
                debugPrint('í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: $e');
              }
            }
          } else if (update.is_done == true) {
            debugPrint('ëª¨ë¸ ì‘ë‹µ ì™„ë£Œ');
            print("\n\n-- done --");
            print("currentResponse: $currentResponse");
            print("utf8response: $utf8response");
            print("jsonresponse: $jsonresponse");
          }
        }
      },
    );

    await llamaLibrary.invoke(
      invokeParametersLlamaLibraryData: InvokeParametersLlamaLibraryData(
        parameters: SendLlamaLibraryMessage.create(
          text: chatHistory_text,
          is_stream: true,
        ),
        isVoid: true,
        extra: null,
        invokeParametersLlamaLibraryDataOptions: null,
      ),
    );
  }

  void dispose() {
    llamaLibrary.dispose();
  }
}

class ChatScreen extends StatefulWidget {
  const ChatScreen({super.key});

  @override
  State<ChatScreen> createState() => _ChatScreenState();
}

class _ChatScreenState extends State<ChatScreen> {
  final TextEditingController _messageController = TextEditingController();
  final List<ChatMessage> _messages = [];
  late LlamaChatService _chatService;
  bool _isLoading = false;
  final LlamaLibraryChatHistory chatHistory = LlamaLibraryChatHistory();
  
  

  @override
  void initState() {
    super.initState();

    chatHistory.clear();

    final systemPrompt = systemPrompt_chat;

    chatHistory.addMessage(
      role: LlamaLibraryRole.system,
      content: jsonDecode(systemPrompt)['content']
    );

    String adjusted_chatHistory = chatHistory.exportFormat(LlamaLibraryChatFormat.alpaca);
    debugPrint("adjusted_chatHistory: $adjusted_chatHistory");

    _chatService = LlamaChatService(
      onResponseUpdate: (response) {
        setState(() {
          if (_messages.isNotEmpty && _messages.last.isAI) {
            _messages.last.content = response;
          }
        });
      },
    );
  }

  @override
  void dispose() {
    _messageController.dispose();
    _chatService.dispose();
    super.dispose();
  }

  void _sendMessage(chatHistory) async {
    final text = _messageController.text.trim();
    if (text.isEmpty) return;

    setState(() {
      _messages.add(ChatMessage(content: text, isAI: false));
      _messages.add(ChatMessage(content: '', isAI: true));
      _isLoading = true;
    });

    _messageController.clear();

    try {
      await _chatService.sendMessage(text, chatHistory);
    } catch (e) {
      print('Error sending message: $e');
    } finally {
      setState(() {
        _isLoading = false;
      });
    }
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('AI ì±„íŒ…'),
      ),
      body: Column(
        children: [
          Expanded(
            child: ListView.builder(
              padding: const EdgeInsets.all(8),
              itemCount: _messages.length,
              itemBuilder: (context, index) {
                final message = _messages[index];
                return Align(
                  alignment: message.isAI
                      ? Alignment.centerLeft
                      : Alignment.centerRight,
                  child: Container(
                    margin: const EdgeInsets.symmetric(vertical: 4),
                    padding: const EdgeInsets.all(12),
                    decoration: BoxDecoration(
                      color: message.isAI ? Colors.grey[200] : Colors.blue[100],
                      borderRadius: BorderRadius.circular(12),
                    ),
                    child: Text(message.content),
                  ),
                );
              },
            ),
          ),
          if (_isLoading)
            const Padding(
              padding: EdgeInsets.all(8.0),
              child: CircularProgressIndicator(),
            ),
          Padding(
            padding: const EdgeInsets.all(8.0),
            child: Row(
              children: [
                Expanded(
                  child: TextField(
                    controller: _messageController,
                    decoration: const InputDecoration(
                      hintText: 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...',
                      border: OutlineInputBorder(),
                    ),
                    onSubmitted: (_) => _sendMessage(chatHistory),
                  ),
                ),
                const SizedBox(width: 8),
                IconButton(
                  onPressed: () => _sendMessage(chatHistory),
                  icon: const Icon(Icons.send),
                ),
              ],
            ),
          ),
        ],
      ),
    );
  }
}

class ChatMessage {
  String content;
  final bool isAI;

  ChatMessage({required this.content, required this.isAI});
}